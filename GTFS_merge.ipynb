{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7c14652",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import shutil\n",
    "\n",
    "def get_gtfs_zip_files(directory):\n",
    "    \"\"\"\n",
    "    指定されたディレクトリ内のZIPファイルを検索する関数\n",
    "    \n",
    "    Args:\n",
    "    directory (str): ZIPファイルが格納されているディレクトリ\n",
    "    \n",
    "    Returns:\n",
    "    list: 見つかったZIPファイルのパスリスト\n",
    "    \"\"\"\n",
    "    zip_files = []\n",
    "    \n",
    "    # ディレクトリ内のファイルをすべて走査\n",
    "    for filename in os.listdir(directory):\n",
    "        # .zipで終わるファイルを選択\n",
    "        if filename.lower().endswith('.zip'):\n",
    "            full_path = os.path.join(directory, filename)\n",
    "            zip_files.append(full_path)\n",
    "    \n",
    "    # ファイルパスをアルファベット順にソート\n",
    "    zip_files.sort()\n",
    "    \n",
    "    return zip_files\n",
    "\n",
    "def extract_gtfs_from_zip(zip_path, extract_dir):\n",
    "    \"\"\"\n",
    "    ZIPファイルを指定されたディレクトリに展開する関数\n",
    "    \n",
    "    Args:\n",
    "    zip_path (str): ZIPファイルパス\n",
    "    extract_dir (str): 展開先ディレクトリ\n",
    "    \n",
    "    Returns:\n",
    "    str: 展開されたディレクトリのパス\n",
    "    \"\"\"\n",
    "    os.makedirs(extract_dir, exist_ok=True)\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_dir)\n",
    "    return extract_dir\n",
    "\n",
    "def get_agency_name(extract_dir):\n",
    "    \"\"\"\n",
    "    展開されたディレクトリからagency_nameを取得する関数\n",
    "    \n",
    "    Args:\n",
    "    extract_dir (str): 展開されたディレクトリのパス\n",
    "    \n",
    "    Returns:\n",
    "    str: agency_name\n",
    "    \"\"\"\n",
    "    agency_path = os.path.join(extract_dir, 'agency.txt')\n",
    "    try:\n",
    "        if os.path.exists(agency_path):\n",
    "            agency_df = pd.read_csv(agency_path)\n",
    "            if 'agency_name' in agency_df.columns and not agency_df.empty:\n",
    "                return agency_df['agency_name'].iloc[0]\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: {agency_path} からagency_nameの取得に失敗しました: {e}\")\n",
    "    \n",
    "    # 取得できない場合はディレクトリ名を返す\n",
    "    return os.path.basename(extract_dir)\n",
    "\n",
    "def merge_gtfs_with_template(template_zip_path, gtfs_directory, output_directory, output_directory_gtfs):\n",
    "    \"\"\"\n",
    "    テンプレートを基準に複数のGTFSフィードをマージする関数\n",
    "    \n",
    "    Args:\n",
    "    template_zip_path (str): テンプレートのZIPファイルパス\n",
    "    gtfs_directory (str): マージするデータのZIPファイルが格納されたディレクトリ\n",
    "    output_directory (str): マージ後のデータを保存するディレクトリ\n",
    "    \"\"\"\n",
    "    # 出力ディレクトリ作成\n",
    "    os.makedirs(output_directory_gtfs, exist_ok=True)\n",
    "    \n",
    "    # ディレクトリ内のZIPファイルを検出\n",
    "    gtfs_zip_paths = get_gtfs_zip_files(gtfs_directory)\n",
    "    print(f\"検出されたZIPファイル: {gtfs_zip_paths}\")\n",
    "    \n",
    "    # 一時ディレクトリ作成\n",
    "    temp_dir = os.path.join(output_directory_gtfs, 'temp')\n",
    "    os.makedirs(temp_dir, exist_ok=True)\n",
    "    \n",
    "    # calendar.txtの曜日カラムリストとtrips.txtの除外カラム\n",
    "    calendar_day_columns = ['monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday']\n",
    "    trips_exclude_columns = ['direction_id']\n",
    "    \n",
    "    try:\n",
    "        # テンプレートZIPを展開\n",
    "        template_extract_dir = extract_gtfs_from_zip(template_zip_path, os.path.join(temp_dir, 'template'))\n",
    "        \n",
    "        # テンプレートファイルのリストを取得\n",
    "        template_files = [f for f in os.listdir(template_extract_dir) \n",
    "                         if f.endswith('.txt') and f != 'fare_rules.txt']\n",
    "        print(f\"テンプレートファイル: {template_files}\")\n",
    "        \n",
    "        # プロバイダー対応表のデータを保持するリスト\n",
    "        mapping_data = []\n",
    "        \n",
    "        # ファイルごとのマージデータを保持する辞書\n",
    "        merged_data = {}\n",
    "        for filename in template_files:\n",
    "            if filename == 'trips.txt':\n",
    "                # trips.txtの場合、direction_idを整数型として読み込む\n",
    "                merged_data[filename] = [pd.read_csv(os.path.join(template_extract_dir, filename), dtype={'direction_id': 'Int64'})]\n",
    "            elif filename == 'routes.txt':\n",
    "                # routes.txtの場合、route_text_colorを文字列として読み込む\n",
    "                merged_data[filename] = [pd.read_csv(os.path.join(template_extract_dir, filename), dtype={'route_text_color': str})]\n",
    "            else:\n",
    "                merged_data[filename] = [pd.read_csv(os.path.join(template_extract_dir, filename))]\n",
    "        \n",
    "        # 各GTFSデータを処理\n",
    "        for i, zip_path in enumerate(gtfs_zip_paths, 1):\n",
    "            # ZIPを展開\n",
    "            provider_dir = extract_gtfs_from_zip(zip_path, os.path.join(temp_dir, f'provider_{i}'))\n",
    "            \n",
    "            # agency_nameを取得\n",
    "            agency_name = get_agency_name(provider_dir)\n",
    "            \n",
    "            # プロバイダー対応表に追加\n",
    "            mapping_data.append({\n",
    "                'provider_id': f'provider_{i}',\n",
    "                'zip_filename': os.path.basename(zip_path),\n",
    "                'agency_name': agency_name\n",
    "            })\n",
    "            \n",
    "            # 各テンプレートファイルについて処理\n",
    "            for filename in template_files:\n",
    "                file_path = os.path.join(provider_dir, filename)\n",
    "                \n",
    "                if os.path.exists(file_path):\n",
    "                    # ファイルの種類に応じて適切なデータ型で読み込む\n",
    "                    if filename == 'trips.txt':\n",
    "                        df = pd.read_csv(file_path, dtype={'direction_id': 'Int64'})\n",
    "                    elif filename == 'routes.txt':\n",
    "                        df = pd.read_csv(file_path, dtype={'route_text_color': str})\n",
    "                    else:\n",
    "                        df = pd.read_csv(file_path)\n",
    "                    \n",
    "                    # テンプレートと同じカラムに揃える\n",
    "                    df = df.reindex(columns=merged_data[filename][0].columns)\n",
    "                    \n",
    "                    # IDカラムがある場合は接頭辞を追加（calendar.txtの曜日カラムとtrips.txtの特定カラムを除外）\n",
    "                    id_columns = [col for col in df.columns \n",
    "                                if 'id' in col.lower() and \n",
    "                                (filename != 'calendar.txt' or col not in calendar_day_columns) and\n",
    "                                (filename != 'trips.txt' or col not in trips_exclude_columns)]\n",
    "                    \n",
    "                    for col in id_columns:\n",
    "                        if col in df.columns:\n",
    "                            if filename == 'trips.txt' and (col == 'block_id' or col == 'shape_id'):\n",
    "                                # block_idとshape_idの場合、空でない値にのみ接頭辞を追加\n",
    "                                df[col] = df[col].apply(lambda x: f'provider_{i}_{x}' if pd.notna(x) and str(x).strip() != '' else x)\n",
    "                            elif filename == 'translations.txt' and (col == 'record_sub_id' or col == 'record_id'):\n",
    "                                # record_idとrecord_sub_idの場合、空でない値にのみ接頭辞を追加\n",
    "                                df[col] = df[col].apply(lambda x: f'provider_{i}_{x}' if pd.notna(x) and str(x).strip() != '' else x)\n",
    "                            else:\n",
    "                                df[col] = f'provider_{i}_' + df[col].astype(str)\n",
    "                    \n",
    "                    # route_text_colorの特別処理\n",
    "                    if filename == 'routes.txt' and 'route_text_color' in df.columns:\n",
    "                        df['route_text_color'] = df['route_text_color'].apply(lambda x: x if pd.isna(x) or str(x).strip() == '' else str(x))\n",
    "                    \n",
    "                    merged_data[filename].append(df)\n",
    "        \n",
    "        # プロバイダー対応表を出力\n",
    "        provider_mapping = pd.DataFrame(mapping_data)\n",
    "        mapping_path = os.path.join(output_directory, 'provider_mapping.txt')\n",
    "        provider_mapping.to_csv(mapping_path, index=False)\n",
    "        print(f\"プロバイダー対応表を {mapping_path} に出力しました\")\n",
    "        \n",
    "        # 各ファイルのマージデータを出力\n",
    "        for filename, dataframes in merged_data.items():\n",
    "            final_merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "            \n",
    "            # trips.txtの場合、direction_idが整数値として出力されるようにする\n",
    "            if filename == 'trips.txt' and 'direction_id' in final_merged_df.columns:\n",
    "                final_merged_df['direction_id'] = final_merged_df['direction_id'].astype('Int64')\n",
    "            \n",
    "            # routes.txtの場合、route_text_colorが文字列として出力され、空欄は空欄のまま保持する\n",
    "            if filename == 'routes.txt' and 'route_text_color' in final_merged_df.columns:\n",
    "                final_merged_df['route_text_color'] = final_merged_df['route_text_color'].apply(\n",
    "                    lambda x: x if pd.isna(x) or str(x).strip() == '' else str(x))\n",
    "            \n",
    "            # translations.txtの場合、指定された5カラムで重複チェックを行い、最初の行のみを残す\n",
    "            if filename == 'translations.txt':\n",
    "                final_merged_df = final_merged_df.drop_duplicates(\n",
    "                    subset=['table_name', 'field_name', 'language', 'field_value'],\n",
    "                    keep='first'\n",
    "                )\n",
    "            \n",
    "            output_path = os.path.join(output_directory_gtfs, filename)\n",
    "            final_merged_df.to_csv(output_path, index=False)\n",
    "            print(f\"{filename} をマージしました\")\n",
    "        \n",
    "        print(\"フィードのマージが完了しました\")\n",
    "    \n",
    "    finally:\n",
    "        # 一時ディレクトリを削除\n",
    "        shutil.rmtree(temp_dir, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4b5b395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "検出されたZIPファイル: ['input/gtfses_kanazawa\\\\20240401flatbus_rev.zip', 'input/gtfses_kanazawa\\\\feed_kaetsunou_kaetsunouippan_20241201_20241106101715.zip', 'input/gtfses_kanazawa\\\\gtfs-hokuriku-railway-20240316_rev.zip', 'input/gtfses_kanazawa\\\\gtfs-hokutetsu2024.zip', 'input/gtfses_kanazawa\\\\hokutetsu.zip', 'input/gtfses_kanazawa\\\\machibus.zip', 'input/gtfses_kanazawa\\\\nishinihonjrbus.zip', 'input/gtfses_kanazawa\\\\notty_current.zip']\n",
      "テンプレートファイル: ['agency.txt', 'calendar.txt', 'calendar_dates.txt', 'feed_info.txt', 'frequencies.txt', 'routes.txt', 'shapes.txt', 'stops.txt', 'stop_times.txt', 'transfers.txt', 'translations.txt', 'trips.txt']\n",
      "プロバイダー対応表を output\\provider_mapping.txt に出力しました\n",
      "agency.txt をマージしました\n",
      "calendar.txt をマージしました\n",
      "calendar_dates.txt をマージしました\n",
      "feed_info.txt をマージしました\n",
      "frequencies.txt をマージしました\n",
      "routes.txt をマージしました\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tachibana\\AppData\\Local\\Temp\\ipykernel_31472\\975870671.py:178: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  final_merged_df = pd.concat(dataframes, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes.txt をマージしました\n",
      "stops.txt をマージしました\n",
      "stop_times.txt をマージしました\n",
      "transfers.txt をマージしました\n",
      "translations.txt をマージしました\n",
      "trips.txt をマージしました\n",
      "フィードのマージが完了しました\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def main():\n",
    "    # テンプレートZIPファイルのパス\n",
    "    template_zip_path = 'GTFS_template/GTFS_template.zip'\n",
    "    \n",
    "    # データのZIPファイルが格納されているディレクトリ\n",
    "    gtfs_directory = 'input/gtfses_kanazawa'\n",
    "    \n",
    "    # 統合されたデータを保存するディレクトリ\n",
    "    output_directory = 'output'\n",
    "    output_directory_gtfs = os.path.join(output_directory, 'merged_gtfs')\n",
    "    \n",
    "    # マージ関数を呼び出し\n",
    "    merge_gtfs_with_template(template_zip_path, gtfs_directory, output_directory, output_directory_gtfs)\n",
    "    \n",
    "    # ZIP化を実行\n",
    "    filename = os.path.join(output_directory, 'merged_gtfs')\n",
    "    shutil.make_archive(filename,'zip',output_directory_gtfs)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311be787",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
